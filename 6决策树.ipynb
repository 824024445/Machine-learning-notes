{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6决策树.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "dJykBXXXUMhH",
        "vyYi5xyBVOO4",
        "uqPv4gZFVJEz",
        "3WuIstCQ8m7j",
        "xhOmKR2lB3Nk",
        "8mVUXQ-YB8BL",
        "V2iB4dUxQLTC",
        "nE05dRa7WGpC"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/824024445/Machine-learning-notes/blob/master/6%E5%86%B3%E7%AD%96%E6%A0%91.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyArwP2TT1ox",
        "colab_type": "text"
      },
      "source": [
        "决策树和支持向量机一样， 决策树是一种多功能机器学习算法， 即可以执行分类任务也可以执行回归任务。  \n",
        "**决策树也是随机森林的基本组成部分，而随机森林是当今最强大的机器学习算法之一。**  \n",
        "本章会同上一节支持向量机一样，先讲述决策树用于分类，然后讲述通过使用 CART 算法，使得决策树执行回归任务"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJykBXXXUMhH",
        "colab_type": "text"
      },
      "source": [
        "# 一、决策树分类\n",
        "仍旧以鸢尾花数据为例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyYi5xyBVOO4",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 实现决策树分类"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oxfh2hETfKy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "00cb230f-8ea6-45cc-9132-490511bc57c3"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris[\"data\"][:, 2:] # 仍旧只取长度和宽度两个特征\n",
        "y = iris[\"target\"]\n",
        "\n",
        "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
        "tree_clf.fit(X, y)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
              "                       max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort=False,\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqPv4gZFVJEz",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 决策树的可视化\n",
        "使用export_graphviz()方法，通过生成一个叫做`iris_tree.dot`的图形定义文件将一个训练好的决策树模型可视化\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwYTXaqeV7mM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import export_graphviz #graphviz为图文的意思\n",
        "\n",
        "export_graphviz(\n",
        " tree_clf,\n",
        " out_file=\"iris_tree.dot\",\n",
        " feature_names=iris.feature_names[2:],\n",
        " class_names=iris.target_names,\n",
        " rounded=True,\n",
        " filled=True\n",
        " )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcchK21iW6Q6",
        "colab_type": "text"
      },
      "source": [
        "然后可以利用`graphviz package` [1] 中的`dot`命令行，将`.dot`文件转换成 PDF 或 PNG 等多种数据格式。例如，使用命令行将`.dot`文件转换成`.png`文件的命令如下：\n",
        "\n",
        "> Graphviz是一款开源图形可视化软件包，<http://www.graphviz.org/>。 在这里下载安装就好了\n",
        "\n",
        "上面的决策树输出图像为：\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FEVzwYPdn5o",
        "colab_type": "text"
      },
      "source": [
        "![替代文字](https://raw.githubusercontent.com/824024445/Machine-learning-notes/master/img/6-1.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WuIstCQ8m7j",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 解析（以1.2为例）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x8CaKS98ve6",
        "colab_type": "text"
      },
      "source": [
        "1. 首先一个决策树包含：\n",
        "  - 一个根节点，一个决策树只包含一个根节点，即第一个节点\n",
        "  - 若干个内部节点（一般的点），如上图中第二行右边的节点\n",
        "  - 若干个叶节点：产生决策结果的节点。如上图中彩色的节点，因为判断出了叶子类别因此是叶节点。\n",
        "2. 每个节点都有一个类别判断class的值。\n",
        "3. 因为在绿色节点中versicolor的样本数占到了48/54=90.74，所以该节点判定这个节点的类别是versicolor。\n",
        "4. 如果该类别的数量与该节点所有样本的数量一致，那么gini==0，gini代表的是纯度。他怎么算呢，以图中绿色节点为例：gini=1-（49/54）^2-(5/49)^2=0.168。\n",
        "5.另外，因为设置的决策树深度为2，所以节点到第二层就停止了，如果设置为3的话，结果如下图"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygaNZET-AaDx",
        "colab_type": "text"
      },
      "source": [
        "![替代文字](https://raw.githubusercontent.com/824024445/Machine-learning-notes/master/img/6-2.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl6ABpdFA1Ez",
        "colab_type": "text"
      },
      "source": [
        "> 注：像决策树这种很容易解释的模型叫做百盒模型，而随机森林或者神经网络等就是黑盒模型了，它们能做很好的预测，但是你很难知道它们的预测过程。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNQGUNRdBN5C",
        "colab_type": "text"
      },
      "source": [
        "6. 输出概率。假设发现了一个花瓣长5cm，宽1.5cm的花朵，预测它分别是三种花的概率\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJL1jMVdBY97",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "32de20bb-446b-48d3-d737-f8bbd25dcefd"
      },
      "source": [
        "tree_clf.predict_proba([[5, 1.5]])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.90740741, 0.09259259]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-x05IqiB0OE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e626096-b96b-45bc-ae53-7076ac9e70e8"
      },
      "source": [
        "tree_clf.predict([[5, 1.5]])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhOmKR2lB3Nk",
        "colab_type": "text"
      },
      "source": [
        "# 二、决策树回归\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mVUXQ-YB8BL",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 CART训练算法\n",
        "CART：全称Classification And Regression Tree分裂回归树  \n",
        "CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b81WyrQACRpT",
        "colab_type": "text"
      },
      "source": [
        "CART算法由以下两步组成：\n",
        "\n",
        "1. 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；  \n",
        "决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准。\n",
        "2. CART决策树的生成就是递归地构建二叉决策树的过程。**CART决策树既可以用于分类也可以用于回归。**本文我们仅讨论用于分类的CART。对分类树而言，CART用Gini系数最小化准则来进行特征选择，生成二叉树。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2iB4dUxQLTC",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 剪枝\n",
        "剪枝是决策树算法应对“过拟合”的主要手段。  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5uTzw2yRD0p",
        "colab_type": "text"
      },
      "source": [
        "剪枝分为预剪枝和后剪枝  \n",
        "预剪枝是：若当前节点的划分不能够带来泛化性能的提升，则不将该节点作为叶节点  \n",
        "后剪枝是：先生成一棵完整的决策树，然后自底向上考察，若将该节点设置为叶节点能够带来性能的提升，则将该节点替换为新的叶节点"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPBeKu53RmeP",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 决策树回归"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJQvFZ_6RrJR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "bfefc1a9-9029-4a01-d446-ba30ffdbd0c4"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg.fit(X, y)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
              "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
              "                      min_impurity_split=None, min_samples_leaf=1,\n",
              "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                      presort=False, random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qZCZ021SdY8",
        "colab_type": "text"
      },
      "source": [
        "![替代文字](https://raw.githubusercontent.com/824024445/Machine-learning-notes/master/img/6-4.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ONuwUe0UECD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "05427a74-835f-4a42-aabc-41de59038d98"
      },
      "source": [
        "tree_reg.predict([[5, 1.5]])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.09259259])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4jdQjkaUFCk",
        "colab_type": "text"
      },
      "source": [
        "可以看出，决策树回归预测的是一个具体的值。预测值为该叶节点相关的所有训练实例的平均目标值。通俗来讲，就是涉及到[5, 1.5]这个样本的判断，有一百多个预测器实例，不只是取最优的决策树，是所有的决策树，它们都对这个点有一个预测值，然后取平均值作为该点的预测值。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_SWd1OVVZm7",
        "colab_type": "text"
      },
      "source": [
        "**另外，CART算法在分类的时候：是以最小gini为目标找决策树  \n",
        "而在回归的时候，是以最小均方误差为目标找决策树**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE05dRa7WGpC",
        "colab_type": "text"
      },
      "source": [
        "# 三、决策树的正则"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lYA_QWrWJEs",
        "colab_type": "text"
      },
      "source": [
        "随着决策树深度的增加，决策树很容易过拟合。所以就需要对模型的形状进行一定的限制，通过设置参数的方式来达到这个目的"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8jFg8b6WnJO",
        "colab_type": "text"
      },
      "source": [
        "`min_samples_split`（节点在被分裂之前必须具有的最小样本数）  \n",
        "`min_samples_leaf`（叶节点必须具有的最小样本数）  \n",
        "`min_weight_fraction_leaf`（和`min_samples_leaf`相同，但表示为加权总数的一小部分实例），`max_leaf_nodes`（叶节点的最大数量）`和max_features`（在每个节点被评估是否分裂的时候，具有的最大特征数量）。增加`min_* hyperparameters`或者减少`max_* hyperparameters`会使模型正则化。\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "一些其他算法的工作原理是在没有任何约束条件下训练决策树模型，让模型自由生长，然后再对不需要的节点进行剪枝。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBqX7ajVXdCE",
        "colab_type": "text"
      },
      "source": [
        "# 四、决策树的缺点\n",
        "决策树最大的缺点就是不稳定。随机森林可以通过多棵树的平均预测值限制这种不稳定性。"
      ]
    }
  ]
}